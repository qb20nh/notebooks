{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yuDiIJJTBlr9","executionInfo":{"status":"ok","timestamp":1663899954919,"user_tz":-540,"elapsed":2,"user":{"displayName":"유태종","userId":"14183084913974218146"}}},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"FZ-DjqgWBlsA"},"source":["# ``torch.autograd``를 사용한 자동 미분\n","\n","신경망을 학습할 때 가장 자주 사용되는 알고리즘은 **역전파(back propagation)**이다. 이 알고리즘에서 매개변수(모델 가중치)는 주어진 매개변수에 대한 손실 함수의 **변화도(gradient)**에\n","따라 조정된다.\n","\n","이러한 변화도를 계산하기 위해 PyTorch에는 ``torch.autograd``라고 불리는 자동 미분 엔진이\n","내장되어 있다. 이는 모든 계산 그래프에 대한 변화도의 자동 계산을 지원한다.\n","\n","자동미분을 이해하기 위해서 입력 ``x``, 매개변수 ``w``와 ``b``, 그리고 손실 함수가 있는 매우 간단한 단일 계층 신경망을 가정해보자:\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QU6TF4o9BlsC","executionInfo":{"status":"ok","timestamp":1663900008956,"user_tz":-540,"elapsed":3695,"user":{"displayName":"유태종","userId":"14183084913974218146"}}},"outputs":[],"source":["import torch\n","\n","x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"]},{"cell_type":"markdown","metadata":{"id":"jDOL-pB7BlsD"},"source":["Tensor, Function과 연산그래프(Computational graph)\n","------------------------------------------------------------------------------------------\n","\n","이 코드는 다음의 **연산 그래프** 를 정의한다:\n","\n","<img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" width=\"800\" height=\"270\">\n","\n","이 신경망에서, ``w``와 ``b``는 최적화를 해야 하는 **매개변수**이다. 따라서\n","이러한 변수들에 대한 손실 함수의 변화도 $\\frac{\\partial loss}{\\partial w}$와 $\\frac{\\partial loss}{\\partial b}$를 계산할 수 있어야 한다. 이를 위해서 해당 텐서에 대해서는\n","``requires_grad`` 속성을 설정하였다."]},{"cell_type":"code","source":["print(x.requires_grad)\n","print(w.requires_grad)\n","print(z.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KduHjnLBhqQn","executionInfo":{"status":"ok","timestamp":1663900558446,"user_tz":-540,"elapsed":375,"user":{"displayName":"유태종","userId":"14183084913974218146"}},"outputId":"bbdaba53-9ed1-4898-8d37-6f78b1538a28"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","True\n","True\n"]}]},{"cell_type":"markdown","metadata":{"id":"7nZ7TWvcBlsE"},"source":["**Note:** ``requires_grad``의 값은 텐서를 생성할 때 설정하거나, ``x.requires_grad_(True)`` 메소드를 사용하여 나중에 설정할 수도 있다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"URP332hlBlsE"},"source":["연산 그래프를 구성하기 위해 텐서에 적용하는 함수는(예를 들어 `matmul`) 사실 ``Function`` 클래스의 객체이다.\n","이 객체는 순전파 방향으로 함수를 계산하는 방법과, 역방향 전파 단계에서 도함수(derivative)를\n","계산하는 방법을 알고 있다. 역방향 전파 함수에 대한 참조(reference)는 텐서의 ``grad_fn``\n","속성에 저장된다. ``Function``에 대한 자세한 정보는\n","이 [문서](<https://pytorch.org/docs/stable/autograd.html#function>)에서 찾아볼 수 있다.\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29Dq9B3zBlsF","executionInfo":{"status":"ok","timestamp":1663900561251,"user_tz":-540,"elapsed":561,"user":{"displayName":"유태종","userId":"14183084913974218146"}},"outputId":"d20f4d5e-a875-4a00-86db-a89a6dd32b42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient function for z = <AddBackward0 object at 0x7fea870855d0>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fea870856d0>\n"]}],"source":["print(f\"Gradient function for z = {z.grad_fn}\")\n","print(f\"Gradient function for loss = {loss.grad_fn}\")"]},{"cell_type":"markdown","metadata":{"id":"w0Pe2TDcBlsF"},"source":["변화도(Gradient) 계산하기\n","-------------------------\n","\n","신경망에서 매개변수의 가중치를 최적화하려면 매개변수에 대한 손실함수의 미분계수(derivative)를\n","계산해야 한다. 즉, 고정된 ``x``와 ``y``에 대해서 $\\frac{\\partial loss}{\\partial w}$와\n","$\\frac{\\partial loss}{\\partial b}$가 필요하다.\n","이러한 도함수를 계산하기 위해, ``loss.backward()`` 를 호출한 다음 ``w.grad``와\n","``b.grad``에서 값을 가져온다:\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91FLKS58BlsG","executionInfo":{"status":"ok","timestamp":1663900563790,"user_tz":-540,"elapsed":339,"user":{"displayName":"유태종","userId":"14183084913974218146"}},"outputId":"868119af-29cd-4fec-cd2e-7073a15215ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.3302, 0.0558, 0.3268],\n","        [0.3302, 0.0558, 0.3268],\n","        [0.3302, 0.0558, 0.3268],\n","        [0.3302, 0.0558, 0.3268],\n","        [0.3302, 0.0558, 0.3268]])\n","tensor([0.3302, 0.0558, 0.3268])\n"]}],"source":["loss.backward()\n","print(w.grad)\n","print(b.grad)"]},{"cell_type":"markdown","metadata":{"id":"_Jyv1hHbBlsG"},"source":["**Note:** \n","  - 연산 그래프의 잎(leaf) 노드들 중 ``requires_grad`` 속성이 ``True``로 설정된\n","    노드들의 ``grad`` 속성만 구할 수 있다. 다른 노드들에서는 변화도를 계산할 수 없다.\n","  - 성능 상의 이유로, 주어진 그래프에서의 ``backward``를 사용한 변화도 계산은 한 번만\n","    수행할 수 있다. 만약 동일한 그래프에서 여러번의 ``backward`` 호출이 필요하면,\n","    ``backward`` 호출 시에 ``retrain_graph=True``를 전달해야 한다."]},{"cell_type":"markdown","metadata":{"id":"z5xy6t3eBlsG"},"source":["변화도 추적 멈추기\n","------------------------------------------------------------------------------------------\n","\n","기본적으로, ``requires_grad=True``인 모든 텐서들은 연산 기록을 추적하고 변화도 계산을\n","지원한다. 그러나 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 *순전파*\n","연산만 필요한 경우에는, 이러한 추적이나 지원이 필요없을 수 있다.\n","연산 코드를 ``torch.no_grad()`` 블록으로 둘러싸서 연산 추적을 멈출 수 있다:\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kyIRJeSMBlsH","executionInfo":{"status":"ok","timestamp":1663900566262,"user_tz":-540,"elapsed":2,"user":{"displayName":"유태종","userId":"14183084913974218146"}},"outputId":"e41e6678-d7cd-4d38-83a5-647d7a532173"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}],"source":["z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"24pNecWzBlsI"},"source":["동일한 결과를 얻는 다른 방법은 텐서에 ``detach()`` 메소드를 사용하는 것이다:\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"rNdi1SuWBlsI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663900567839,"user_tz":-540,"elapsed":2,"user":{"displayName":"유태종","userId":"14183084913974218146"}},"outputId":"840517a6-1c29-4d6c-ce15-e891f10efb12"},"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}],"source":["z = torch.matmul(x, w)+b\n","z_det = z.detach()\n","print(z_det.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"vf6cfsZFBlsJ"},"source":["변화도 추적을 멈춰야 하는 이유들은 다음과 같다:\n","  - 신경망의 일부 매개변수를 **고정된 매개변수(frozen parameter)**로 만들고 싶은 경우이다. 이는\n","    사전 학습된 신경망을 [미세조정(fine tuning)](<https://tutorials.pytorch.kr/beginner/finetuning_torchvision_models_tutorial.html>)\n","    할 때 매우 일반적인 시나리오이다.\n","  - 변화도를 추적하지 않는 텐서의 연산이 더 효율적이기 때문에, 순전파 단계만 수행할 때\n","    **연산 속도가 향상된다.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q1QhBH90BlsJ"},"source":["연산 그래프\n","------------------------------------------------------------------------------------------\n","\n","개념적으로, autograd는 데이터(텐서), 실행된 모든 연산들, 그리고 연산의 결과인 새로운 텐서에 대한\n","기록을 [`Function`](<https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function>) 객체들로 구성된 방향성 비순환 그래프(DAG; Directed Acyclic Graph)에 저장한다.\n","이 DAG의 잎(leave)은 입력 텐서이고, 뿌리(root)는 출력 텐서이다.\n","이 그래프를 뿌리에서부터 잎까지 추적하면 연쇄 법칙(chain rule)에 따라 변화도를 자동으로 계산할 수 있다.\n","\n","순전파 단계에서, autograd는 다음 두 가지 작업을 동시에 수행한다:\n","\n","- 요청된 연산을 수행하여 결과 텐서를 계산하고,\n","- DAG에 연산의 *변화도 함수(gradient function)*를 유지(maintain)한다.\n","\n","역전파 단계는 ``.backward()``가 호출될 때 DAG의 뿌리(root)에서 시작된다. ``autograd``는 이 때:\n","\n","- 각 ``.grad_fn``으로부터 변화도를 계산하고,\n","- 각 텐서의 ``.grad`` 속성에 계산 결과를 누적하고(accumulate),\n","- 연쇄 법칙을 사용하여, 모든 잎(leaf) 텐서들까지 전파(propagate)한다.\n","\n","\n","**Note:** **PyTorch에서 DAG들은 동적(dynamic)이다.**\n","주목해야 할 중요한 점은 그래프가 처음부터(from scratch) 다시 생성된다는 것이다; 매번 ``.bachward()``가 호출되고 나면, autograd는 새로운 그래프를 만들기(populate) 시작한다. 이러한 점 덕분에 모델에서 흐름 제어(control flow) 구문들을 사용할 수 있게 되는 것이다; 매번 반복(iteration)할 때마다 필요하면 모양(shape)이나 크기(size), 연산(operation)을 바꿀 수도 있다. which means you can turn on and off parts of network as you go\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gO5FolLtBlsK"},"source":["텐서 변화도와 야코비안 곱 (Jacobian Product)\n","------------------------------------------------------------------------------------------\n","\n","대부분의 경우에 신경망에서 손실함수는 스칼라 함수이다.\n","그러나 손실 함수의 출력이 임의의 텐서인 경우가 있다. 이럴 때, PyTorch는 실제 변화도가 아닌\n","**야코비안 곱(Jacobian product)**을 계산한다.\n","\n","Jacobian Matrix: input vector -> output vector all possible partial derivatives\n","\n","$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$이고,\n","$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$일 때\n","벡터 함수 $\\vec{y}=f(\\vec{x})$에서 $\\vec{x}$에 대한\n","$\\vec{y}$의 변화도는 **야코비안 행렬(Jacobian matrix)**로 주어진다:\n","\n","\\begin{align}J=\\left(\\begin{array}{ccc}\n","      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n","      \\vdots & \\ddots & \\vdots\\\\\n","      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","      \\end{array}\\right)\\end{align}\n","\n","야코비안 행렬 자체를 계산하는 대신, PyTorch는 주어진 입력 벡터 $v=(v_1 \\dots v_m)$에 대한\n","**야코비안 곱(Jacobian Product)**  $v^T\\cdot J$을 계산한다.\n","이 과정은 $v$를 인자로 ``backward``를 호출하면 이루어진다.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DysZ46VqBlsL","executionInfo":{"status":"ok","timestamp":1663027589998,"user_tz":-540,"elapsed":280,"user":{"displayName":"권오흠","userId":"05475008821310211864"}},"outputId":"197dfd89-1649-47a5-bf91-7712ec7f5873"},"outputs":[{"output_type":"stream","name":"stdout","text":["First call\n","tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.],\n","        [2., 2., 2., 2., 4.]])\n","\n","Second call\n","tensor([[8., 4., 4., 4., 4.],\n","        [4., 8., 4., 4., 4.],\n","        [4., 4., 8., 4., 4.],\n","        [4., 4., 4., 8., 4.],\n","        [4., 4., 4., 4., 8.]])\n","\n","Call after zeroing gradients\n","tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.],\n","        [2., 2., 2., 2., 4.]])\n"]}],"source":["inp = torch.eye(5, requires_grad=True)\n","out = (inp+1).pow(2)\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(f\"First call\\n{inp.grad}\")\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(f\"\\nSecond call\\n{inp.grad}\")\n","inp.grad.zero_()\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"]},{"cell_type":"markdown","metadata":{"id":"peuQsWWNBlsL"},"source":["동일한 인자로 ``backward``를 두차례 호출하면 변화도 값이 달라진다.\n","이는 역방향 전파를 수행할 때, PyTorch가 **변화도를 누적(accumulate)해두기 때문**\n","이다. 즉, 계산된 변화도의 값이 연산 그래프의 모든 잎(leaf) 노드의 ``grad`` 속성에\n","더해진다. 따라서 제대로된 변화도를 계산하기 위해서는 ``grad`` 속성을 먼저 0으로 만들어야\n","한다. 실제 학습 과정에서는 **옵티마이저(optimizer)**가 이 과정을 도와준다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IXHdGI7gBlsM"},"source":["**Note:** 이전에는 매개변수 없이 ``backward()`` 함수를 호출했다. 이는 본질적으로\n","``backward(torch.tensor(1.0))``을 호출하는 것과 동일하며,\n","신경망 훈련 중의 손실과 같은 스칼라-값 함수의 변화도를 계산하는 유용한 방법이다.\n"]},{"cell_type":"markdown","metadata":{"id":"os5YWxSqBlsM"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NcwcXWOeBlsM"},"source":["### 더 읽어보기\n","\n","- [Autograd Mechanics](<https://pytorch.org/docs/stable/notes/autograd.html>)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[{"file_id":"1Z8YZG7_Sr4LzBmdYb6TFK7kPQP3SbhsO","timestamp":1663899771690}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}